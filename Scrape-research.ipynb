{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9c9e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (4.13.5)\n",
      "Requirement already satisfied: duckduckgo-search in c:\\users\\risha\\anaconda3\\lib\\site-packages (8.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: primp>=0.15.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from duckduckgo-search) (0.15.0)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from duckduckgo-search) (8.1.8)\n",
      "Requirement already satisfied: lxml>=5.3.0 in c:\\users\\risha\\anaconda3\\lib\\site-packages (from duckduckgo-search) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\risha\\appdata\\roaming\\python\\python310\\site-packages (from click>=8.1.8->duckduckgo-search) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c925c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching workshop page: https://openreview.net/group?id=neurips.cc/2024/workshop/safegenai#tab-accept-oral\n",
      "Found 0 papers on the page.\n",
      "Could not find any authors. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# --- Configuration ---\n",
    "WORKSHOP_URL = \"https://openreview.net/group?id=neurips.cc/2024/workshop/safegenai#tab-accept-oral\"\n",
    "OUTPUT_CSV_FILE = \"researchers_safegenai_2024.csv\"\n",
    "REQUEST_DELAY = 1  # Seconds to wait between requests to be polite to the server\n",
    "\n",
    "# --- Headers to mimic a browser ---\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_authors_from_workshop(url):\n",
    "    \"\"\"\n",
    "    Scrapes the main workshop page to get a list of papers and their authors.\n",
    "    Returns a list of dictionaries, each containing author name, paper title, and profile URL.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching workshop page: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching workshop URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    papers = soup.find_all('div', class_='note')\n",
    "    \n",
    "    author_list = []\n",
    "    print(f\"Found {len(papers)} papers on the page.\")\n",
    "\n",
    "    for paper in papers:\n",
    "        # Get paper title\n",
    "        title_tag = paper.find('h4', class_='note-title')\n",
    "        paper_title = title_tag.get_text(strip=True) if title_tag else \"Unknown Title\"\n",
    "\n",
    "        # Get authors\n",
    "        authors_div = paper.find('div', class_='note-authors')\n",
    "        if not authors_div:\n",
    "            continue\n",
    "        \n",
    "        author_links = authors_div.find_all('a')\n",
    "        for link in author_links:\n",
    "            author_name = link.get_text(strip=True)\n",
    "            # Filter out anonymous authors\n",
    "            if \"Anonymous\" in author_name:\n",
    "                continue\n",
    "            \n",
    "            profile_url_suffix = link.get('href')\n",
    "            author_info = {\n",
    "                'name': author_name,\n",
    "                'paper_title': paper_title,\n",
    "                'profile_url': f\"https://openreview.net{profile_url_suffix}\" if profile_url_suffix else None\n",
    "            }\n",
    "            author_list.append(author_info)\n",
    "            \n",
    "    return author_list\n",
    "\n",
    "def find_homepage(author_info):\n",
    "    \"\"\"\n",
    "    Tries to find the homepage for a single author.\n",
    "    First checks their OpenReview profile, then falls back to a web search.\n",
    "    \"\"\"\n",
    "    name = author_info['name']\n",
    "    profile_url = author_info['profile_url']\n",
    "\n",
    "    # Strategy 1: Check OpenReview Profile (High-Confidence)\n",
    "    if profile_url:\n",
    "        try:\n",
    "            time.sleep(REQUEST_DELAY) # Polite delay\n",
    "            response = requests.get(profile_url, headers=HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                profile_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # OpenReview profiles often have a div with id 'homepage'\n",
    "                homepage_div = profile_soup.find('div', id='homepage')\n",
    "                if homepage_div and homepage_div.find('a'):\n",
    "                    homepage_url = homepage_div.find('a').get('href')\n",
    "                    print(f\"  [SUCCESS] Found homepage for {name} on OpenReview profile.\")\n",
    "                    return homepage_url\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  [WARN] Could not fetch OpenReview profile for {name}: {e}\")\n",
    "\n",
    "    # Strategy 2: Fallback to DuckDuckGo Search (Best-Guess)\n",
    "    print(f\"  [INFO] Searching DuckDuckGo for {name}'s homepage...\")\n",
    "    try:\n",
    "        time.sleep(REQUEST_DELAY) # Polite delay\n",
    "        query = f'\"{name}\" AI researcher homepage OR personal website'\n",
    "        search_results = list(DDGS().text(query, max_results=1))\n",
    "        \n",
    "        if search_results:\n",
    "            homepage_guess = search_results[0]['href']\n",
    "            print(f\"  [GUESS] Found potential homepage for {name} via search.\")\n",
    "            return homepage_guess\n",
    "        else:\n",
    "            print(f\"  [FAIL] No homepage found for {name} via search.\")\n",
    "            return \"Not Found\"\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] DuckDuckGo search failed for {name}: {e}\")\n",
    "        return \"Search Failed\"\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Saves the final data to a CSV file.\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Author Name', 'Paper Title', 'Homepage (Best Guess)']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for item in data:\n",
    "            writer.writerow({\n",
    "                'Author Name': item['name'],\n",
    "                'Paper Title': item['paper_title'],\n",
    "                'Homepage (Best Guess)': item['homepage']\n",
    "            })\n",
    "    print(f\"\\nSuccessfully saved data for {len(data)} researchers to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Get all authors from the workshop page\n",
    "    authors = get_authors_from_workshop(WORKSHOP_URL)\n",
    "    \n",
    "    if not authors:\n",
    "        print(\"Could not find any authors. Exiting.\")\n",
    "    else:\n",
    "        # Use a dictionary to avoid processing the same author multiple times\n",
    "        unique_authors = {author['profile_url']: author for author in authors if author['profile_url']}\n",
    "        print(f\"\\nFound {len(authors)} author mentions, corresponding to {len(unique_authors)} unique profiles.\")\n",
    "        \n",
    "        final_results = []\n",
    "        \n",
    "        # Step 2: Find homepage for each unique author\n",
    "        for i, author_info in enumerate(unique_authors.values()):\n",
    "            print(f\"\\nProcessing author {i+1}/{len(unique_authors)}: {author_info['name']}\")\n",
    "            homepage = find_homepage(author_info)\n",
    "            \n",
    "            # Add the found homepage to our data\n",
    "            result = author_info.copy()\n",
    "            result['homepage'] = homepage\n",
    "            final_results.append(result)\n",
    "\n",
    "        # Step 3: Save all collected data to a CSV\n",
    "        save_to_csv(final_results, OUTPUT_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a91630ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/SafeGenAi#tab-accept-oral'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "headings = soup.find_all('div',class_='note')\n",
    "for heading in headings:\n",
    "    print(heading.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97809a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== AI Researcher Security Pipeline Starting ======\n",
      "This tool scrapes researcher data and scans their public code for secrets.\n",
      "Please use responsibly and ethically.\n",
      "--- Checking Prerequisites ---\n",
      "✅ Git and Gitleaks are found.\n",
      "✅ Creating Gitleaks config file: generated_gitleaks_config.toml\n",
      "\n",
      "--- Phase 1: Scraping Authors from OpenReview ---\n",
      "Fetching: https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/SafeGenAi#tab-accept-oral\n",
      "Found 0 unique authors.\n",
      "No authors found. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# --- Configuration ---\n",
    "WORKSHOP_URL = \"https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/SafeGenAi#tab-accept-oral\"\n",
    "FINAL_REPORT_CSV = \"consolidated_secrets_report.csv\"\n",
    "TEMP_CLONE_DIR = \"temp_pipeline_clones\"\n",
    "GITLEAKS_CONFIG_FILE = \"generated_gitleaks_config.toml\"\n",
    "REQUEST_DELAY = 1  # Seconds to wait between network requests to be polite\n",
    "\n",
    "# This TOML content will be written to the Gitleaks config file automatically.\n",
    "GITLEAKS_CUSTOM_RULES_CONTENT = \"\"\"\n",
    "# Gitleaks custom configuration file for LLM API keys\n",
    "# This file is generated automatically by the pipeline script.\n",
    "\n",
    "[[rules]]\n",
    "  id = \"openai-api-key\"\n",
    "  description = \"OpenAI API Key detected\"\n",
    "  regex = '''sk-(proj-)?([a-zA-Z0-9]{20,70})'''\n",
    "  tags = [\"api\", \"key\", \"llm\", \"openai\"]\n",
    "  keywords = [\"openai\", \"sk-\"]\n",
    "\n",
    "[[rules]]\n",
    "  id = \"anthropic-api-key\"\n",
    "  description = \"Anthropic API Key detected\"\n",
    "  regex = '''sk-ant-api\\\\d{2}-[\\\\w-]{95}'''\n",
    "  tags = [\"api\", \"key\", \"llm\", \"anthropic\"]\n",
    "  keywords = [\"anthropic\", \"sk-ant-\"]\n",
    "\n",
    "[[rules]]\n",
    "  id = \"litellm-proxy-key\"\n",
    "  description = \"LiteLLM Proxy Key detected\"\n",
    "  regex = '''sk-litellm-[a-zA-Z0-9]{24,64}'''\n",
    "  tags = [\"api\", \"key\", \"llm\", \"litellm\"]\n",
    "  keywords = [\"litellm\", \"sk-litellm-\"]\n",
    "\n",
    "[[rules]]\n",
    "  id = \"fireworks-ai-api-key\"\n",
    "  description = \"Fireworks AI API Key detected\"\n",
    "  regex = '''fw-[a-zA-Z0-9]{48}'''\n",
    "  tags = [\"api\", \"key\", \"llm\", \"fireworksai\"]\n",
    "  keywords = [\"fireworks\", \"fw-\"]\n",
    "\n",
    "[[rules]]\n",
    "  id = \"together-ai-api-key\"\n",
    "  description = \"Together AI API Key detected\"\n",
    "  regex = '''[a-fA-F0-9]{64}'''\n",
    "  keywords = [\"TOGETHER_API_KEY\", \"together_api_key\", \"together-api-key\"]\n",
    "  tags = [\"api\", \"key\", \"llm\", \"togetherai\"]\n",
    "\"\"\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- Utility and Prerequisite Functions ---\n",
    "\n",
    "def check_prerequisites():\n",
    "    \"\"\"Checks if git and gitleaks are installed.\"\"\"\n",
    "    print(\"--- Checking Prerequisites ---\")\n",
    "    if not shutil.which(\"git\"):\n",
    "        print(\"FATAL: 'git' command not found. Please install Git and ensure it's in your PATH.\")\n",
    "        return False\n",
    "    if not shutil.which(\"gitleaks\"):\n",
    "        print(\"FATAL: 'gitleaks' command not found. Please install Gitleaks and ensure it's in your PATH.\")\n",
    "        return False\n",
    "    print(\"✅ Git and Gitleaks are found.\")\n",
    "    return True\n",
    "\n",
    "def create_gitleaks_config():\n",
    "    \"\"\"Creates the gitleaks config file from the embedded string.\"\"\"\n",
    "    print(f\"✅ Creating Gitleaks config file: {GITLEAKS_CONFIG_FILE}\")\n",
    "    with open(GITLEAKS_CONFIG_FILE, \"w\") as f:\n",
    "        f.write(GITLEAKS_CUSTOM_RULES_CONTENT)\n",
    "\n",
    "# --- Phase 1: Data Scraping Functions ---\n",
    "\n",
    "def scrape_authors_from_workshop(url):\n",
    "    print(f\"\\n--- Phase 1: Scraping Authors from OpenReview ---\")\n",
    "    print(f\"Fetching: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"FATAL: Error fetching workshop URL: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    papers = soup.find_all('div', class_='note')\n",
    "    unique_authors = {}\n",
    "    for paper in papers:\n",
    "        authors_div = paper.find('div', class_='note-authors')\n",
    "        if not authors_div: continue\n",
    "        for link in authors_div.find_all('a'):\n",
    "            author_name = link.get_text(strip=True)\n",
    "            profile_suffix = link.get('href')\n",
    "            if \"Anonymous\" in author_name or not profile_suffix: continue\n",
    "            profile_url = f\"https://openreview.net{profile_suffix}\"\n",
    "            if profile_url not in unique_authors:\n",
    "                unique_authors[profile_url] = {'name': author_name, 'profile_url': profile_url}\n",
    "    \n",
    "    print(f\"Found {len(unique_authors)} unique authors.\")\n",
    "    return list(unique_authors.values())\n",
    "\n",
    "def find_researcher_details(author_list):\n",
    "    print(\"\\n--- Phase 2: Finding Homepages and GitHub Profiles ---\")\n",
    "    enriched_authors = []\n",
    "    for i, author in enumerate(author_list):\n",
    "        print(f\"\\n({i+1}/{len(author_list)}) Processing: {author['name']}\")\n",
    "        \n",
    "        # Find Homepage\n",
    "        homepage_url = find_homepage(author)\n",
    "        author['homepage'] = homepage_url\n",
    "        \n",
    "        # Find GitHub Profile\n",
    "        github_profile = find_github_profile(author['name'], homepage_url)\n",
    "        author['github_profile'] = github_profile\n",
    "\n",
    "        # Find Repositories\n",
    "        if github_profile != \"Not Found\":\n",
    "            repos = get_repos_from_github_api(github_profile)\n",
    "            author['repos'] = repos\n",
    "        else:\n",
    "            author['repos'] = \"N/A\"\n",
    "        \n",
    "        enriched_authors.append(author)\n",
    "    return enriched_authors\n",
    "\n",
    "def find_homepage(author):\n",
    "    profile_url = author['profile_url']\n",
    "    if profile_url:\n",
    "        try:\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            response = requests.get(profile_url, headers=HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                homepage_div = soup.find('div', id='homepage')\n",
    "                if homepage_div and homepage_div.find('a'):\n",
    "                    print(f\"  -> Found homepage on OpenReview profile.\")\n",
    "                    return homepage_div.find('a').get('href')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  [WARN] Could not fetch profile {profile_url}: {e}\")\n",
    "    \n",
    "    print(f\"  -> Searching for homepage for {author['name']}...\")\n",
    "    try:\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        query = f'\"{author[\"name\"]}\" AI researcher homepage OR personal website'\n",
    "        results = list(DDGS().text(query, max_results=1))\n",
    "        return results[0]['href'] if results else \"Not Found\"\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] DuckDuckGo search failed: {e}\")\n",
    "        return \"Search Failed\"\n",
    "\n",
    "def find_github_profile(name, homepage):\n",
    "    if homepage and homepage.startswith('http'):\n",
    "        try:\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "            response = requests.get(homepage, headers=HEADERS, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    if 'github.com/' in link['href']:\n",
    "                        match = re.search(r'(https://github\\.com/[^/]+)', link['href'])\n",
    "                        if match:\n",
    "                            print(f\"  -> Found GitHub profile on homepage.\")\n",
    "                            return match.group(1)\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass # Fail silently and proceed to search\n",
    "    \n",
    "    print(f\"  -> Searching for GitHub profile for {name}...\")\n",
    "    try:\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        query = f'\"{name}\" site:github.com'\n",
    "        results = list(DDGS().text(query, max_results=1))\n",
    "        if results and 'github.com' in results[0]['href']:\n",
    "            match = re.search(r'(https://github\\.com/[^/]+)', results[0]['href'])\n",
    "            return match.group(1) if match else \"Not Found\"\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] DuckDuckGo search failed: {e}\")\n",
    "    return \"Not Found\"\n",
    "\n",
    "def get_repos_from_github_api(profile_url):\n",
    "    match = re.search(r'github\\.com/([^/]+)', profile_url)\n",
    "    if not match: return \"Invalid Profile URL\"\n",
    "    username = match.group(1)\n",
    "    api_url = f\"https://api.github.com/users/{username}/repos?per_page=100\"\n",
    "    print(f\"  -> Fetching repos for '{username}' from GitHub API...\")\n",
    "    try:\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        response = requests.get(api_url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code == 403: return \"API Rate Limit Exceeded\"\n",
    "        response.raise_for_status()\n",
    "        repos = response.json()\n",
    "        repo_names = [repo['name'] for repo in repos if not repo['fork']]\n",
    "        return \"; \".join(repo_names) if repo_names else \"No Public Repos Found\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"API Request Failed\"\n",
    "\n",
    "# --- Phase 2: Scanning Functions ---\n",
    "\n",
    "def scan_repositories(researchers):\n",
    "    print(\"\\n--- Phase 3: Cloning and Scanning Repositories ---\")\n",
    "    all_findings = []\n",
    "    \n",
    "    if os.path.exists(TEMP_CLONE_DIR):\n",
    "        shutil.rmtree(TEMP_CLONE_DIR)\n",
    "    os.makedirs(TEMP_CLONE_DIR)\n",
    "\n",
    "    for researcher in researchers:\n",
    "        org_name = researcher['github_profile'].split('/')[-1] if researcher['github_profile'] != \"Not Found\" else \"Unknown\"\n",
    "        repos_str = researcher.get('repos', '')\n",
    "        \n",
    "        if not repos_str or repos_str in [\"N/A\", \"No Public Repos Found\", \"API Rate Limit Exceeded\"]:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nScanning repos for {researcher['name']} ({org_name})\")\n",
    "        repo_names = [r.strip() for r in repos_str.split(';')]\n",
    "\n",
    "        for repo_name in repo_names:\n",
    "            repo_url = f\"https://github.com/{org_name}/{repo_name}\"\n",
    "            local_repo_path = os.path.join(TEMP_CLONE_DIR, repo_name)\n",
    "            \n",
    "            print(f\"  Cloning {repo_url}...\")\n",
    "            clone_result = subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", repo_url, local_repo_path], capture_output=True, text=True)\n",
    "            if clone_result.returncode != 0:\n",
    "                print(f\"  [FAIL] Could not clone. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Scanning with Gitleaks...\")\n",
    "            report_file = os.path.join(TEMP_CLONE_DIR, \"report.json\")\n",
    "            subprocess.run([\n",
    "                \"gitleaks\", \"detect\",\n",
    "                \"--config\", GITLEAKS_CONFIG_FILE,\n",
    "                \"--source\", local_repo_path,\n",
    "                \"--report-path\", report_file,\n",
    "                \"--report-format\", \"json\",\n",
    "                \"--no-git\"\n",
    "            ], capture_output=True)\n",
    "\n",
    "            if os.path.exists(report_file) and os.path.getsize(report_file) > 0:\n",
    "                with open(report_file, 'r') as f:\n",
    "                    findings = json.load(f)\n",
    "                print(f\"  [SUCCESS] Found {len(findings)} potential secrets in {repo_name}.\")\n",
    "                for finding in findings:\n",
    "                    all_findings.append({\n",
    "                        'Organization Name': org_name,\n",
    "                        'Person Name': researcher['name'],\n",
    "                        'API Key (Secret)': finding['Secret'],\n",
    "                        'Associated File Location': f\"https://github.com/{org_name}/{repo_name}/blob/{finding['Commit']}/{finding['File']}\"\n",
    "                    })\n",
    "            \n",
    "            shutil.rmtree(local_repo_path) # Clean up repo immediately\n",
    "\n",
    "    return all_findings\n",
    "\n",
    "# --- Main Orchestrator ---\n",
    "\n",
    "def main():\n",
    "    print(\"====== AI Researcher Security Pipeline Starting ======\")\n",
    "    print(\"This tool scrapes researcher data and scans their public code for secrets.\")\n",
    "    print(\"Please use responsibly and ethically.\")\n",
    "    \n",
    "    if not check_prerequisites():\n",
    "        return\n",
    "    \n",
    "    create_gitleaks_config()\n",
    "\n",
    "    # Phase 1 & 2: Scrape and enrich data\n",
    "    authors = scrape_authors_from_workshop(WORKSHOP_URL)\n",
    "    if not authors:\n",
    "        print(\"No authors found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    researchers_with_repos = find_researcher_details(authors)\n",
    "\n",
    "    # Phase 3: Scan repositories\n",
    "    final_findings = scan_repositories(researchers_with_repos)\n",
    "\n",
    "    # Phase 4: Report and Clean up\n",
    "    print(\"\\n--- Phase 4: Generating Final Report and Cleaning Up ---\")\n",
    "    if final_findings:\n",
    "        print(f\"✅ Found a total of {len(final_findings)} potential secrets.\")\n",
    "        with open(FINAL_REPORT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['Organization Name', 'Person Name', 'API Key (Secret)', 'Associated File Location'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(final_findings)\n",
    "        print(f\"✅ Final report saved to: {FINAL_REPORT_CSV}\")\n",
    "    else:\n",
    "        print(\"✅ No secrets were found across all scanned repositories.\")\n",
    "\n",
    "    # Cleanup\n",
    "    if os.path.exists(TEMP_CLONE_DIR):\n",
    "        shutil.rmtree(TEMP_CLONE_DIR)\n",
    "    if os.path.exists(GITLEAKS_CONFIG_FILE):\n",
    "        os.remove(GITLEAKS_CONFIG_FILE)\n",
    "    print(\"✅ All temporary files and directories have been removed.\")\n",
    "    print(\"\\n====== Pipeline Finished. ======\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844c99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed26a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
